{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fDKKvVCe0zX",
        "outputId": "8c8544b3-9913-4384-bb0e-3758e8d7342f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "                      input                     target\n",
            "0    카메라을 헬스장에 두고 온 것 같습니다.      카메라을 헬스장에 두고 온 것 같노ㅎㅎ\n",
            "1  설명을 조금 더 자세히 해 주실 수 있나요?  설명을 쪼끔 더 자세히 해 주실 수 있노?ㅎㅎ\n",
            "2          카페에 저녁에 도착하겠습니다.              카페에 저녁에 도착하겠노\n",
            "3     카메라을 버정에 두고 온 것 같습니다.         카메라을 버정에 두고 온 것 같누\n",
            "4      카메라 배터리가 너무 빨리 닳습니다.        카메라 배터리가 너무 빨리 닳노ㄷㄷ\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "path = \"/content/drive/MyDrive/slang_pairs_proper_v2.csv\"\n",
        "\n",
        "df = pd.read_csv(path)\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42, shuffle=True)\n",
        "\n",
        "PREFIX = \"slang: \"\n",
        "X_train = (PREFIX + train_df[\"input\"].astype(str)).tolist()\n",
        "y_train = train_df[\"target\"].astype(str).tolist()\n",
        "X_val   = (PREFIX + val_df[\"input\"].astype(str)).tolist()\n",
        "y_val   = val_df[\"target\"].astype(str).tolist()"
      ],
      "metadata": {
        "id": "eAfkplE2ocxU"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델/토커나이저 준비"
      ],
      "metadata": {
        "id": "WWqRsiFMfR0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM\n",
        "\n",
        "model_name = \"google/mt5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name, from_pt=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PyIc41sfSVN",
        "outputId": "7d65876f-dae5-41dc-e95c-df0d13d30545"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/tf_keras/src/initializers/initializers.py:121: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
            "  warnings.warn(\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFMT5ForConditionalGeneration: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight']\n",
            "- This IS expected if you are initializing TFMT5ForConditionalGeneration from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFMT5ForConditionalGeneration from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFMT5ForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMT5ForConditionalGeneration for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "max_in, max_out = 128, 64\n",
        "\n",
        "enc_train = tokenizer(\n",
        "    X_train, max_length=max_in, truncation=True, padding=\"max_length\", return_tensors=\"tf\"\n",
        ")\n",
        "enc_val = tokenizer(\n",
        "    X_val,   max_length=max_in, truncation=True, padding=\"max_length\", return_tensors=\"tf\"\n",
        ")\n",
        "\n",
        "labels_train = tokenizer(\n",
        "    text_target=y_train, max_length=max_out, truncation=True, padding=\"max_length\", return_tensors=\"tf\"\n",
        ")[\"input_ids\"]\n",
        "labels_val = tokenizer(\n",
        "    text_target=y_val,   max_length=max_out, truncation=True, padding=\"max_length\", return_tensors=\"tf\"\n",
        ")[\"input_ids\"]\n",
        "\n",
        "# pad → -100 (loss에서 무시)\n",
        "labels_train = tf.where(labels_train == tokenizer.pad_token_id, -100, labels_train)\n",
        "labels_val   = tf.where(labels_val   == tokenizer.pad_token_id, -100, labels_val)\n",
        "\n",
        "train_features = dict(enc_train); train_features[\"labels\"] = labels_train\n",
        "val_features   = dict(enc_val);    val_features[\"labels\"]   = labels_val\n",
        "\n",
        "BATCH = 16  # T4면 8~16 권장 (OOM 나면 더 줄이기)\n",
        "train_ds = tf.data.Dataset.from_tensor_slices(train_features).shuffle(10000).batch(BATCH).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds   = tf.data.Dataset.from_tensor_slices(val_features).batch(BATCH).prefetch(tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "ZKeYdsMnoREA"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델 학습"
      ],
      "metadata": {
        "id": "eM8lla65qlxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import create_optimizer\n",
        "import math\n",
        "\n",
        "EPOCHS = 5\n",
        "steps_per_epoch = math.ceil(len(train_df) / BATCH)\n",
        "num_train_steps = steps_per_epoch * EPOCHS\n",
        "num_warmup_steps = int(num_train_steps * 0.1)\n",
        "\n",
        "optimizer, _ = create_optimizer(\n",
        "    init_lr=5e-5,\n",
        "    num_train_steps=num_train_steps,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    weight_decay_rate=0.01\n",
        ")\n",
        "\n",
        "# TF Seq2Seq는 loss를 내부 compute_loss로 계산 → loss 지정 불필요\n",
        "model.compile(optimizer=optimizer)\n",
        "history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y06Mrst3oyGO",
        "outputId": "c6c6e4e5-117a-4122-ac97-feb6e52cd141"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "102/102 [==============================] - 105s 643ms/step - loss: 19.4791 - val_loss: 7.1564\n",
            "Epoch 2/5\n",
            "102/102 [==============================] - 61s 595ms/step - loss: 12.0049 - val_loss: 5.4712\n",
            "Epoch 3/5\n",
            "102/102 [==============================] - 60s 588ms/step - loss: 9.9283 - val_loss: 4.7989\n",
            "Epoch 4/5\n",
            "102/102 [==============================] - 61s 594ms/step - loss: 8.9474 - val_loss: 4.4479\n",
            "Epoch 5/5\n",
            "102/102 [==============================] - 60s 589ms/step - loss: 8.3873 - val_loss: 4.3034\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 저장"
      ],
      "metadata": {
        "id": "br-D78Jnf7kk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained('seq2seq_model/bert-base')\n",
        "tokenizer.save_pretrained('seq2seq_model/bert-base')"
      ],
      "metadata": {
        "id": "X9_OklzFf7R1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 추론"
      ],
      "metadata": {
        "id": "eqiVnYzvj2Vt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from transformers import pipeline, GenerationConfig\n",
        "\n",
        "# (<extra_id_*>) 금지\n",
        "bad_words_ids = [tokenizer.encode(f\"<extra_id_{i}>\", add_special_tokens=False) for i in range(100)]\n",
        "\n",
        "# 안정 디코딩 프리셋(빔서치)\n",
        "gen_cfg = GenerationConfig(\n",
        "    max_new_tokens=48,\n",
        "    do_sample=False,              # 안정: 샘플링 OFF\n",
        "    num_beams=4,\n",
        "    length_penalty=0.9,\n",
        "    no_repeat_ngram_size=3,\n",
        "    repetition_penalty=1.15,\n",
        "    bad_words_ids=bad_words_ids,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "gen = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, framework=\"tf\")\n",
        "\n",
        "def enforce_no_nu(text, is_q=False):\n",
        "    s = text.strip()\n",
        "    s = re.sub(r\"[.!?]+$\", \"\", s)\n",
        "    if re.search(r\"(노\\??|누\\??)$\", s):\n",
        "        return s\n",
        "    return s + (\"노?\" if is_q else \"노\")\n",
        "\n",
        "def slangify(sentence):\n",
        "    is_q = sentence.strip().endswith(\"?\")\n",
        "    # ✅ return_full_text 인자 제거\n",
        "    out = gen(PREFIX + sentence, generation_config=gen_cfg)[0][\"generated_text\"]\n",
        "    return enforce_no_nu(out, is_q)\n",
        "\n",
        "print(slangify(\"회의 자료를 공유해 주시겠어요?\"))\n",
        "print(slangify(\"오늘 날씨가 정말 좋습니다.\"))\n",
        "print(slangify(\"내일까지 결과를 보내 주십시오.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfWkvyqZj3kf",
        "outputId": "cfb135d4-d30f-4dce-f3f3-45234f4b0d2d"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "...! 회의 자료를 공유해주시겠어요노?\n",
            "....”오늘 날씨가 좋습니다노\n",
            "ddi 결과를 내일 내일까지을 결과로 보내 주십시오노\n"
          ]
        }
      ]
    }
  ]
}